{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa51160c-81d4-4c23-a4a8-3e76d3aa9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevents the Windows timezone error\n",
    "import time\n",
    "time.tzname = ('Europe/Vienna', 'Europe/Vienna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1965b167-5f73-495d-a329-460db79f7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Provenance safety switch ----\n",
    "#PROVENANCE_ENABLED = True # set to True when server is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12502707'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '058'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12502840'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12502707'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4080a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontologies used: foaf, prov, IAO #skip\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Brikenda\" .',\n",
    "f':{student_a} foaf:familyName \"Lajqi Pepshi\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12502840\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Sofiana\" .',\n",
    "f':{student_b} foaf:familyName \"Braho\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12502707\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "890a782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/processed/colleges_usnews.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initial Setup: ARFF -> CSV Conversion\n",
    "\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "raw_arff_path = os.path.join(\"data\", \"colleges_usnews.arff\")\n",
    "processed_dir = os.path.join(\"data\", \"processed\")\n",
    "csv_path = os.path.join(processed_dir, \"colleges_usnews.csv\")\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    data, meta = arff.loadarff(raw_arff_path)\n",
    "    df_raw = pd.DataFrame(data)\n",
    "\n",
    "    # decode byte strings\n",
    "    for col in df_raw.select_dtypes(include=[\"object\"]):\n",
    "        df_raw[col] = df_raw[col].str.decode(\"utf-8\")\n",
    "\n",
    "    df_raw.to_csv(csv_path, index=False)\n",
    "\n",
    "csv_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase #skip\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The selected dataset originates from OpenML (ID 538) and contains information about colleges in the United States, including educational, financial, and demographic attributes. The data was originally published in the U.S. News and World Report directory. It includes characteristics such as tuition levels, student-to-faculty ratio, admission figures, test score ranges, and institutional type.\n",
    "\n",
    "A realistic scenario for this dataset is an analysis project conducted by an educational consultancy. The consultancy aims to understand how different college attributes influence accessibility, academic environment, and student decision-making. The dataset provides a broad overview of institutional features, enabling investigations into factors that may support strategic planning for applicants or policy discussions for institutions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The main business objective is to support informed decision-making for prospective students, parents, and educational advisors by analyzing relationships between institutional characteristics and overall student experience indicators. The analysis aims to uncover helpful patterns in the data, such as which attributes tend to be associated with higher graduation success, stronger academic environments, or affordability.\n",
    "\n",
    "A further objective is to assist advisory services in identifying groups of institutions with similar characteristics, enabling targeted recommendations based on student preferences and needs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "Success is achieved if the analysis provides useful insights that are understandable and helpful for advisory and planning tasks. Examples of successful outcomes include identifying influential attributes, forming meaningful institution groupings, or offering helpful summaries that guide decision-making.\n",
    "\n",
    "The project is considered successful if the findings enable stakeholders to interpret patterns in the dataset in a way that supports practical guidance, such as highlighting accessible institutions or identifying colleges with characteristics that align with specific academic goals.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The data mining goals focus on preparing, examining, and modeling the dataset to identify meaningful relationships between college characteristics. Goals include creating a reliable preprocessing pipeline, conducting structured exploration of the attributes, identifying potential patterns, and preparing the processed data for later modeling activities.\n",
    "\n",
    "Depending on the properties of the dataset after preparation, the goals may include predicting selected indicators or grouping institutions based on their characteristics to support interpretation and comparison.\n",
    "4\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "Data mining success is achieved when the preprocessing steps result in a consistent and usable dataset, when attribute exploration provides interpretable results, and when initial models or clustering attempts behave in a stable and reproducible manner.\n",
    "\n",
    "Success is also measured by how well the analytical outcomes support the business goals, such as producing interpretable summary statistics, helpful visualizations, or structure in the data that stakeholders can rely on during later decision-making.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Potential risks arise from the possibility of bias related to institutional type, regional differences, and economic factors. Some attributes may indirectly represent sensitive or structural inequalities, which requires cautious interpretation. There is also a risk that incomplete or outdated data may lead to imbalanced conclusions if not handled appropriately.\n",
    "\n",
    "To reduce these risks, the analysis will pay attention to inconsistent values, uneven attribute distributions, and potential skews. The later modeling steps will require careful evaluation to ensure that outcomes do not unintentionally reinforce disadvantage among particular types of institutions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"d84b5383-101d-4735-adcb-d658581a6a04\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FICE</th>\n",
       "      <th>College_name</th>\n",
       "      <th>State</th>\n",
       "      <th>Public/private_indicator</th>\n",
       "      <th>Average_Math_SAT_score</th>\n",
       "      <th>Average_Verbal_SAT_score</th>\n",
       "      <th>Average_Combined_SAT_score</th>\n",
       "      <th>Average_ACT_score</th>\n",
       "      <th>First_quartile-Math_SAT</th>\n",
       "      <th>Third_quartile-Math_SAT</th>\n",
       "      <th>...</th>\n",
       "      <th>Board_costs</th>\n",
       "      <th>Additional_fees</th>\n",
       "      <th>Estimated_book_costs</th>\n",
       "      <th>Estimated_personal_spending</th>\n",
       "      <th>Pct._of_faculty_with_Ph.D.s</th>\n",
       "      <th>Pct._of_faculty_with_terminal_degree</th>\n",
       "      <th>Student/faculty_ratio</th>\n",
       "      <th>Pct.alumni_who_donate</th>\n",
       "      <th>Instructional_expenditure_per_student</th>\n",
       "      <th>Graduation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1061.0</td>\n",
       "      <td>Alaska_Pacific_University</td>\n",
       "      <td>AK</td>\n",
       "      <td>2.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10922.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063.0</td>\n",
       "      <td>University_of_Alaska_at_Fairbanks</td>\n",
       "      <td>AK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11935.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1065.0</td>\n",
       "      <td>University_of_Alaska_Southeast</td>\n",
       "      <td>AK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9584.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11462.0</td>\n",
       "      <td>University_of_Alaska_at_Anchorage</td>\n",
       "      <td>AK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8046.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002.0</td>\n",
       "      <td>Alabama_Agri__and__Mech_Univ</td>\n",
       "      <td>AL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7043.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FICE                       College_name State  Public/private_indicator  \\\n",
       "0   1061.0          Alaska_Pacific_University    AK                       2.0   \n",
       "1   1063.0  University_of_Alaska_at_Fairbanks    AK                       1.0   \n",
       "2   1065.0     University_of_Alaska_Southeast    AK                       1.0   \n",
       "3  11462.0  University_of_Alaska_at_Anchorage    AK                       1.0   \n",
       "4   1002.0       Alabama_Agri__and__Mech_Univ    AL                       1.0   \n",
       "\n",
       "   Average_Math_SAT_score  Average_Verbal_SAT_score  \\\n",
       "0                   490.0                     482.0   \n",
       "1                   499.0                     462.0   \n",
       "2                     NaN                       NaN   \n",
       "3                   459.0                     422.0   \n",
       "4                     NaN                       NaN   \n",
       "\n",
       "   Average_Combined_SAT_score  Average_ACT_score  First_quartile-Math_SAT  \\\n",
       "0                       972.0               20.0                    440.0   \n",
       "1                       961.0               22.0                      NaN   \n",
       "2                         NaN                NaN                      NaN   \n",
       "3                       881.0               20.0                      NaN   \n",
       "4                         NaN               17.0                      NaN   \n",
       "\n",
       "   Third_quartile-Math_SAT  ...  Board_costs  Additional_fees  \\\n",
       "0                    530.0  ...       2500.0            130.0   \n",
       "1                      NaN  ...       1790.0            155.0   \n",
       "2                      NaN  ...       2250.0             34.0   \n",
       "3                      NaN  ...       2520.0            114.0   \n",
       "4                      NaN  ...       1442.0            155.0   \n",
       "\n",
       "   Estimated_book_costs  Estimated_personal_spending  \\\n",
       "0                 800.0                       1500.0   \n",
       "1                 650.0                       2304.0   \n",
       "2                 500.0                       1162.0   \n",
       "3                 580.0                       1260.0   \n",
       "4                 500.0                        850.0   \n",
       "\n",
       "   Pct._of_faculty_with_Ph.D.s  Pct._of_faculty_with_terminal_degree  \\\n",
       "0                         76.0                                  72.0   \n",
       "1                         67.0                                   NaN   \n",
       "2                         39.0                                  51.0   \n",
       "3                         48.0                                   NaN   \n",
       "4                         53.0                                  53.0   \n",
       "\n",
       "   Student/faculty_ratio  Pct.alumni_who_donate  \\\n",
       "0                  119.0                    2.0   \n",
       "1                  100.0                    8.0   \n",
       "2                   95.0                    NaN   \n",
       "3                  137.0                    6.0   \n",
       "4                  143.0                    NaN   \n",
       "\n",
       "   Instructional_expenditure_per_student  Graduation_rate  \n",
       "0                                10922.0             15.0  \n",
       "1                                11935.0              NaN  \n",
       "2                                 9584.0             39.0  \n",
       "3                                 8046.0              NaN  \n",
       "4                                 7043.0             40.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "universities_data_path = os.path.join(\"data\", \"processed\")\n",
    "load_university_data_code_writer = student_a\n",
    "\n",
    "def load_university_data()-> pd.DataFrame:\n",
    "\n",
    "    input_file = os.path.join(universities_data_path, 'colleges_usnews.csv')\n",
    "    raw_data = pd.read_csv(\n",
    "        input_file,\n",
    "        sep=',',\n",
    "        engine='python',\n",
    "        on_bad_lines='skip'\n",
    "    )\n",
    "    \n",
    "    loaded_data = raw_data\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_university_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "ld_ass_uuid_executor = \"cf0471d5-173c-49c0-843f-4dee15e1652f\"\n",
    "load_university_data_executor = [\n",
    "    f':load_university_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(load_university_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"ff0caa59-00d7-482c-b4ff-5a7aed2abf21\"\n",
    "ld_report = \"\"\"\n",
    "Load the US News college/university dataset from CSV into a pandas DataFrame.\n",
    "The CSV is parsed with the python engine and skips malformed lines to ensure loading succeeds.\n",
    "\"\"\"\n",
    "load_university_data_activity = [\n",
    "    ':load_university_data rdf:type prov:Activity .',\n",
    "    ':load_university_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_university_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_university_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_university_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_university_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_university_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_university_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':load_university_data prov:used :raw_data .',\n",
    "    ':load_university_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_university_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(load_university_data_activity, prefixes=prefixes)\n",
    "\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'US News College/University dataset\\' .',\n",
    "    ':raw_data sc:description \\'Dataset containing information about US colleges/universities and ranking-related indicators.\\' .',\n",
    "    ':universities_csv rdf:type cr:FileObject .',\n",
    "    ':universities_csv sc:name \\'colleges_usnews.csv\\' .',\n",
    "    ':universities_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :universities_csv .',\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Table of college/university indicators\\' .',\n",
    "    ':raw_recordset cr:source :universities_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "]\n",
    "\n",
    "cols = list(data.columns)[:6]\n",
    "\n",
    "for c in cols:\n",
    "    safe = str(c).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    raw_data_triples += [\n",
    "        f':raw_recordset cr:field :field_{safe} .',\n",
    "        f':field_{safe} rdf:type cr:Field .',\n",
    "        f':field_{safe} sc:name \\'{c}\\' .',\n",
    "        f':field_{safe} sc:description \\'Column {c} from the dataset.\\' .',\n",
    "        f':field_{safe} cr:dataType xsd:string .',\n",
    "    ]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':data sc:name \\'Loaded US News College/University dataset\\' .',\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':recordset sc:name \\'Loaded table of college/university indicators\\' .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    safe = str(c).replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    data_triples += [\n",
    "        f':recordset cr:field :field_{safe} .',\n",
    "    ]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c7ac995-4966-4f9c-8c4c-be42f1fe9fa5",
   "metadata": {},
   "source": [
    "Missing values are represented as NaN following conversion from ARFF to CSV using pandas, which enables consistent handling of incomplete observations during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0580e18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average_Math_SAT_score': [{'index': 54, 'z_score': 3.5852759002752426}, {'index': 76, 'z_score': 2.4057253095433637}, {'index': 77, 'z_score': 3.4378320764337578}, {'index': 79, 'z_score': 2.848056781067818}, {'index': 97, 'z_score': 3.378854546897164}, {'index': 152, 'z_score': 2.2582814857018785}, {'index': 308, 'z_score': 2.4057253095433637}, {'index': 354, 'z_score': 2.4057253095433637}, {'index': 432, 'z_score': 2.626891045305591}, {'index': 455, 'z_score': 3.4673208412020546}, {'index': 483, 'z_score': 2.2582814857018785}, {'index': 495, 'z_score': 2.641635427689739}, {'index': 648, 'z_score': -2.754808524908608}, {'index': 654, 'z_score': 2.656379810073888}, {'index': 715, 'z_score': 2.73010172199463}, {'index': 741, 'z_score': 2.966011840141006}, {'index': 781, 'z_score': 2.5531691333848485}, {'index': 784, 'z_score': 3.4378320764337578}, {'index': 795, 'z_score': -2.3124770533841534}, {'index': 863, 'z_score': -2.4599208772256382}, {'index': 964, 'z_score': 2.5384247510007}, {'index': 1021, 'z_score': 2.4352140743116606}, {'index': 1024, 'z_score': 2.5531691333848485}, {'index': 1040, 'z_score': 2.5531691333848485}, {'index': 1069, 'z_score': -2.4599208772256382}, {'index': 1129, 'z_score': -2.518898406762232}, {'index': 1154, 'z_score': -2.4451764948414896}, {'index': 1155, 'z_score': -2.607364701067123}, {'index': 1219, 'z_score': -2.4156877300731927}], 'Average_Verbal_SAT_score': [{'index': 30, 'z_score': -2.3195132010677457}, {'index': 54, 'z_score': 3.4096307606593035}, {'index': 76, 'z_score': 2.3804432226245043}, {'index': 77, 'z_score': 2.895036991641904}, {'index': 79, 'z_score': 3.066568247981037}, {'index': 97, 'z_score': 3.49539638882887}, {'index': 152, 'z_score': 2.7235057353027705}, {'index': 251, 'z_score': 2.4147494738923307}, {'index': 308, 'z_score': 2.3804432226245043}, {'index': 432, 'z_score': 3.0494151223471238}, {'index': 455, 'z_score': 3.0494151223471238}, {'index': 478, 'z_score': 2.3804432226245043}, {'index': 495, 'z_score': 2.483361976427984}, {'index': 511, 'z_score': 2.208911966285371}, {'index': 648, 'z_score': -3.1085569802277586}, {'index': 654, 'z_score': 2.637740107133204}, {'index': 715, 'z_score': 2.843577614740164}, {'index': 741, 'z_score': 3.18664012741843}, {'index': 781, 'z_score': 2.7235057353027705}, {'index': 782, 'z_score': 2.3804432226245043}, {'index': 784, 'z_score': 2.3804432226245043}, {'index': 822, 'z_score': 2.208911966285371}, {'index': 851, 'z_score': 2.431902599526244}, {'index': 863, 'z_score': -2.4224319548712256}, {'index': 895, 'z_score': 2.311830720088851}, {'index': 900, 'z_score': -2.6282694624781855}, {'index': 951, 'z_score': 2.431902599526244}, {'index': 961, 'z_score': 2.895036991641904}, {'index': 1021, 'z_score': 2.895036991641904}, {'index': 1024, 'z_score': 2.2775244688210243}, {'index': 1040, 'z_score': 2.7235057353027705}, {'index': 1065, 'z_score': -2.2509006985320927}, {'index': 1069, 'z_score': -2.2509006985320927}, {'index': 1151, 'z_score': -2.216594447264266}, {'index': 1154, 'z_score': -2.491044457406879}, {'index': 1155, 'z_score': -2.9370257238886253}, {'index': 1188, 'z_score': 2.3804432226245043}, {'index': 1219, 'z_score': -2.4738913317729656}], 'Average_Combined_SAT_score': [{'index': 30, 'z_score': -2.217055640340518}, {'index': 54, 'z_score': 3.57687968325135}, {'index': 76, 'z_score': 2.443987301543443}, {'index': 77, 'z_score': 3.2531961456205196}, {'index': 79, 'z_score': 3.0104334923973965}, {'index': 97, 'z_score': 3.5040508872844134}, {'index': 152, 'z_score': 2.5249081859511504}, {'index': 251, 'z_score': 2.233593002083403}, {'index': 308, 'z_score': 2.443987301543443}, {'index': 432, 'z_score': 2.880960077345064}, {'index': 455, 'z_score': 3.3422091184689977}, {'index': 495, 'z_score': 2.6220132472403996}, {'index': 648, 'z_score': -2.97771195377297}, {'index': 654, 'z_score': 2.7029341316481075}, {'index': 715, 'z_score': 2.8404996351412106}, {'index': 741, 'z_score': 3.131814819008958}, {'index': 781, 'z_score': 2.686749954766566}, {'index': 784, 'z_score': 3.0104334923973965}, {'index': 863, 'z_score': -2.4921866473267245}, {'index': 900, 'z_score': -2.3303448785113092}, {'index': 961, 'z_score': 2.363066417135735}, {'index': 1021, 'z_score': 2.7029341316481075}, {'index': 1024, 'z_score': 2.476355655306526}, {'index': 1040, 'z_score': 2.686749954766566}, {'index': 1069, 'z_score': -2.4112657629190166}, {'index': 1154, 'z_score': -2.5164629126490365}, {'index': 1155, 'z_score': -2.815870184957555}, {'index': 1188, 'z_score': 2.20122464832032}, {'index': 1219, 'z_score': -2.4921866473267245}], 'Average_ACT_score': [{'index': 30, 'z_score': -2.3723596484885485}, {'index': 76, 'z_score': 2.2789853099210364}, {'index': 79, 'z_score': 3.4418215495234326}, {'index': 97, 'z_score': 3.4418215495234326}, {'index': 152, 'z_score': 2.666597389788502}, {'index': 171, 'z_score': -2.3723596484885485}, {'index': 209, 'z_score': 2.2789853099210364}, {'index': 251, 'z_score': 2.666597389788502}, {'index': 293, 'z_score': 2.2789853099210364}, {'index': 308, 'z_score': 2.666597389788502}, {'index': 354, 'z_score': 3.054209469655967}, {'index': 455, 'z_score': 3.4418215495234326}, {'index': 483, 'z_score': 2.2789853099210364}, {'index': 495, 'z_score': 3.054209469655967}, {'index': 629, 'z_score': -2.3723596484885485}, {'index': 630, 'z_score': -2.3723596484885485}, {'index': 653, 'z_score': 2.666597389788502}, {'index': 654, 'z_score': 3.054209469655967}, {'index': 817, 'z_score': 2.2789853099210364}, {'index': 846, 'z_score': 2.2789853099210364}, {'index': 847, 'z_score': 2.2789853099210364}, {'index': 849, 'z_score': 2.2789853099210364}, {'index': 895, 'z_score': 2.666597389788502}, {'index': 951, 'z_score': 2.2789853099210364}, {'index': 964, 'z_score': 2.2789853099210364}, {'index': 1024, 'z_score': 2.666597389788502}, {'index': 1040, 'z_score': 2.2789853099210364}, {'index': 1069, 'z_score': -2.3723596484885485}, {'index': 1117, 'z_score': 2.2789853099210364}, {'index': 1129, 'z_score': -2.3723596484885485}, {'index': 1154, 'z_score': -4.310420047825875}, {'index': 1155, 'z_score': -2.3723596484885485}], 'Board_costs': [{'index': 80, 'z_score': 2.7034945646967032}, {'index': 151, 'z_score': 2.476820154651421}, {'index': 156, 'z_score': 2.8697224653965767}, {'index': 160, 'z_score': 3.7477080136386363}, {'index': 162, 'z_score': 2.6400257298840244}, {'index': 163, 'z_score': 2.2199224899334347}, {'index': 282, 'z_score': 3.4590759315143105}, {'index': 414, 'z_score': -2.3120545482385726}, {'index': 436, 'z_score': 3.6706387142232404}, {'index': 439, 'z_score': 2.673271310023999}, {'index': 784, 'z_score': 2.9301689747419855}, {'index': 803, 'z_score': 2.2123666762652587}, {'index': 820, 'z_score': 3.1568433847872677}, {'index': 909, 'z_score': 3.096396875441859}, {'index': 932, 'z_score': -2.2077843196177427}, {'index': 963, 'z_score': 6.330285125421217}, {'index': 1024, 'z_score': 2.2803689992788434}, {'index': 1234, 'z_score': 4.8493456464587075}, {'index': 1236, 'z_score': 2.2501457446061393}, {'index': 1263, 'z_score': 4.078652652304748}]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PROVENANCE_ENABLED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     50\u001b[39m co_ass_uuid_executor = \u001b[33m\"\u001b[39m\u001b[33m15085e9d-15f1-4727-9b6e-776dd07fcd08\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m check_outliers_executor = [\n\u001b[32m     52\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:check_outliers prov:qualifiedAssociation :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mco_ass_uuid_executor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     53\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mco_ass_uuid_executor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prov:agent :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     54\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mco_ass_uuid_executor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Association .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mco_ass_uuid_executor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prov:hadRole :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     56\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPROVENANCE_ENABLED\u001b[49m:\n\u001b[32m     58\u001b[39m     engine.insert(check_outliers_executor, prefixes=prefixes)\n\u001b[32m     60\u001b[39m co_ass_uuid_writer = \u001b[33m\"\u001b[39m\u001b[33mcd4970df-9f40-4bb1-8fad-e4dc4fcdd284\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'PROVENANCE_ENABLED' is not defined"
     ]
    }
   ],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, threshold=3.0, columns=()) -> dict:\n",
    "    results = {}\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        # temporary numeric conversion for Data Understanding (does not permanently modify dataset)\n",
    "        values = pd.to_numeric(tmp[col], errors=\"coerce\").dropna()\n",
    "\n",
    "        if values.empty:\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "        mask = np.abs(z_scores) > threshold\n",
    "\n",
    "        outlier_info = [\n",
    "            {\"index\": int(idx), \"z_score\": float(z_scores.loc[idx])}\n",
    "            for idx in values[mask].index\n",
    "        ]\n",
    "        results[col] = outlier_info\n",
    "\n",
    "    return results\n",
    "\n",
    "numeric_cols = [\n",
    "    \"Average_Math_SAT_score\",\n",
    "    \"Average_Verbal_SAT_score\",\n",
    "    \"Average_Combined_SAT_score\",\n",
    "    \"Average_ACT_score\",\n",
    "    \"Board_costs\",\n",
    "]\n",
    "\n",
    "start_time_co = now()\n",
    "outliers_report = check_outliers(data, threshold=2.2, columns=numeric_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1) Activity: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"15085e9d-15f1-4727-9b6e-776dd07fcd08\"\n",
    "check_outliers_executor = [\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "if PROVENANCE_ENABLED:\n",
    "    engine.insert(check_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "co_comment = \"\"\"\n",
    "Outliers were screened using z-scores per numeric attribute. A threshold of 2.2 was applied during exploratory\n",
    "analysis; flagged observations will be reviewed in the data preparation phase before any removal is applied.\n",
    "\"\"\"\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:comment \"Data Understanding\" .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{check_outliers_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_outliers prov:used :data .',\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outliers_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "]\n",
    "if PROVENANCE_ENABLED:\n",
    "    engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "# 2) Activity: Inspecting the report and taking a decision\n",
    "ior_ass_uuid_executor = \"6eaa2c0a-e592-4d85-b37f-d695844910cf\"\n",
    "ior_comment = \"\"\"\n",
    "The outlier report was inspected. Potential outliers will be handled in the data preparation phase\n",
    "(e.g., verification, correction, or removal depending on context).\n",
    "\"\"\"\n",
    "inspect_outlier_report_executor = student_a\n",
    "\n",
    "inspect_outlier_report_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':inspect_outlier_report rdfs:comment \"Data Understanding\" .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{ior_comment}\"\"\" .',\n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_ho}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:endedAtTime \"{end_time_ho}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_outlier_report_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    ':outlier_decision rdfs:comment \"Outliers flagged for review; handling deferred to Data Preparation.\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "]\n",
    "if PROVENANCE_ENABLED:\n",
    "    engine.insert(inspect_outlier_report_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94dfb7-328c-432b-b7e7-1f66f03eabca",
   "metadata": {},
   "source": [
    "**Checking the structure, quality, distribution, skewness, plausibility of values and outliners**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique states: 51\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Categorical structure and duplicates\n",
    "print(\"Unique states:\", data[\"State\"].nunique())\n",
    "print(\"Duplicate rows:\", data.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     779.000000\n",
       "mean      967.978177\n",
       "std       123.577493\n",
       "min       600.000000\n",
       "25%       884.500000\n",
       "50%       957.000000\n",
       "75%      1038.000000\n",
       "max      1410.000000\n",
       "Name: Average_Combined_SAT_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution check (Data Understanding)\n",
    "sat = pd.to_numeric(\n",
    "    data[\"Average_Combined_SAT_score\"],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "sat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2431af5-8720-4841-b3e2-1fc287381514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAT skewness: 0.5296870415650089\n",
      "ACT skewness: 0.31664729895765986\n"
     ]
    }
   ],
   "source": [
    "# Skewness check (Data Understanding)\n",
    "# Purpose: Inspect the asymmetry of numeric attribute distributions.\n",
    "# This helps identify whether variables are skewed, which may influence\n",
    "# later modeling decisions. No data transformation is applied at this stage.\n",
    "\n",
    "sat = pd.to_numeric(\n",
    "    data[\"Average_Combined_SAT_score\"],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "act = pd.to_numeric(\n",
    "    data[\"Average_ACT_score\"],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"SAT skewness:\", sat.skew())\n",
    "print(\"ACT skewness:\", act.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18e055dc-6c3d-4968-9689-562a65a4389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAT min: 600.0 SAT max: 1410.0\n",
      "ACT min: 11.0 ACT max: 31.0\n"
     ]
    }
   ],
   "source": [
    "# Plausibility checks (Data Understanding)\n",
    "print(\"SAT min:\", sat.min(), \"SAT max:\", sat.max())\n",
    "\n",
    "act = pd.to_numeric(\n",
    "    data[\"Average_ACT_score\"],\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "print(\"ACT min:\", act.min(), \"ACT max:\", act.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab370f2a-a9b4-44b9-baf5-9bd204e2a9ff",
   "metadata": {},
   "source": [
    "The distribution of SAT and ACT scores shows right-skewness, with a small number of extreme values identified in the upper and lower tails. All observed values fall within plausible ranges for standardized tests (SAT and ACT), indicating no obvious data entry errors. Skewed distributions and potential outliers are expected in educational and cost-related datasets and will be addressed in the Data Preparation phase if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bcb438d-c884-40fe-89e8-89a773e40b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Third_quartile-ACT                                 639\n",
       "First_quartile-ACT                                 639\n",
       "Average_ACT_score                                  588\n",
       "First_quartile-Math_SAT                            530\n",
       "Third_quartile-Verbal_SAT                          530\n",
       "Third_quartile-Math_SAT                            530\n",
       "First_quartile-Verbal_SAT                          530\n",
       "Average_Math_SAT_score                             525\n",
       "Average_Verbal_SAT_score                           525\n",
       "Average_Combined_SAT_score                         523\n",
       "Board_costs                                        498\n",
       "Room_costs                                         321\n",
       "Additional_fees                                    274\n",
       "Pct._new_students_from_top_10Perc_of_H.S._class    235\n",
       "Pct.alumni_who_donate                              222\n",
       "Pct._new_students_from_top_25Perc_of_H.S._class    202\n",
       "Estimated_personal_spending                        181\n",
       "Graduation_rate                                     98\n",
       "Room_and_board_costs                                76\n",
       "Estimated_book_costs                                48\n",
       "Instructional_expenditure_per_student               39\n",
       "Number_of_parttime_undergraduates                   32\n",
       "Pct._of_faculty_with_Ph.D.s                         32\n",
       "In-state_tuition                                    30\n",
       "Pct._of_faculty_with_terminal_degree                30\n",
       "Out-of-state_tuition                                20\n",
       "Number_of_applicants_accepted                       11\n",
       "Number_of_applications_received                     10\n",
       "Number_of_new_students_enrolled                      5\n",
       "Number_of_fulltime_undergraduates                    3\n",
       "Student/faculty_ratio                                2\n",
       "State                                                0\n",
       "College_name                                         0\n",
       "Public/private_indicator                             0\n",
       "FICE                                                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect missing values (NaN after ARFF -> CSV conversion)\n",
    "data.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb614e-2c50-4f86-a379-041fc7a18fd6",
   "metadata": {},
   "source": [
    "Based on the results of the data understanding phase, several implications for data preparation and modeling were identified. Multiple numerical attributes, such as tuition-related and expenditure-related variables, exhibit skewed distributions and large value ranges, indicating that scaling or transformation may be required to ensure stable model behavior. Additionally, several attributes related to admissions and test scores contain missing values, which motivates the use of appropriate imputation strategies during data preparation.\n",
    "\n",
    "Correlation analysis revealed that some institutional attributes are highly correlated, particularly among admission and selectivity-related variables. This suggests that feature selection or dimensionality reduction may be necessary to reduce redundancy and prevent multicollinearity in subsequent modeling steps. Furthermore, the presence of mixed data types (numerical and categorical) indicates that encoding strategies must be applied consistently before model training.\n",
    "\n",
    "Overall, these findings highlight the need for a structured and reproducible preprocessing pipeline and motivate the selection of modeling approaches that are robust to correlated features, varying scales, and incomplete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "    ':data_preparation_phase rdf:type prov:Activity .',\n",
    "    ':data_preparation_phase rdfs:label \"Data Preparation Phase\" .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)Outlier Handling (Decision Execution - Keep Outliers)\n",
    "#Action based on Data Understanding decision\n",
    "\n",
    "handle_outliers_code_writer = student_b\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Outliers identified during the Data Understanding phase are deliberately\n",
    "    retained at this stage. Extreme values may correspond to meaningful\n",
    "    institutions (e.g. very selective or large universities), and removing\n",
    "    them prematurely could distort the underlying data distribution.\n",
    "    \"\"\"\n",
    "    return df\n",
    "\n",
    "start_time_td = now()\n",
    "cleaned_data = handle_outliers(data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case, by deliberately not removing detected outliers\n",
    "\n",
    "ro_ass_uuid_executor = \"8bf41348-a777-486c-b64d-6ef7a917efc0\"\n",
    "\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"8b0426e3-e9c7-4311-90f6-2c0e268e9272\"\n",
    "\n",
    "td_comment = \"\"\"\n",
    "Outliers were identified during the Data Understanding phase using\n",
    "statistical inspection techniques. After reviewing these results,\n",
    "a decision was made not to remove any observations at this stage.\n",
    "\n",
    "The rationale is that extreme values may represent real and relevant\n",
    "institutions, such as highly selective or exceptionally large universities.\n",
    "Outlier removal is therefore deferred to later modeling stages, where\n",
    "algorithm-specific robustness considerations can be applied.\n",
    "\"\"\"\n",
    "\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \"Data Preparation\" .',\n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .',\n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29af7fc7-27b7-4660-82d7-bfa3c9d1531f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FICE                                               0\n",
       "College_name                                       0\n",
       "State                                              0\n",
       "Public/private_indicator                           0\n",
       "Average_Math_SAT_score                             0\n",
       "Average_Verbal_SAT_score                           0\n",
       "Average_Combined_SAT_score                         0\n",
       "Average_ACT_score                                  0\n",
       "First_quartile-Math_SAT                            0\n",
       "Third_quartile-Math_SAT                            0\n",
       "First_quartile-Verbal_SAT                          0\n",
       "Third_quartile-Verbal_SAT                          0\n",
       "First_quartile-ACT                                 0\n",
       "Third_quartile-ACT                                 0\n",
       "Number_of_applications_received                    0\n",
       "Number_of_applicants_accepted                      0\n",
       "Number_of_new_students_enrolled                    0\n",
       "Pct._new_students_from_top_10Perc_of_H.S._class    0\n",
       "Pct._new_students_from_top_25Perc_of_H.S._class    0\n",
       "Number_of_fulltime_undergraduates                  0\n",
       "Number_of_parttime_undergraduates                  0\n",
       "In-state_tuition                                   0\n",
       "Out-of-state_tuition                               0\n",
       "Room_and_board_costs                               0\n",
       "Room_costs                                         0\n",
       "Board_costs                                        0\n",
       "Additional_fees                                    0\n",
       "Estimated_book_costs                               0\n",
       "Estimated_personal_spending                        0\n",
       "Pct._of_faculty_with_Ph.D.s                        0\n",
       "Pct._of_faculty_with_terminal_degree               0\n",
       "Student/faculty_ratio                              0\n",
       "Pct.alumni_who_donate                              0\n",
       "Instructional_expenditure_per_student              0\n",
       "Graduation_rate                                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Value Handling (Transformation)\n",
    "\n",
    "handle_missing_code_writer = student_b\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Missing numeric values are imputed using the median of each attribute.\n",
    "    This choice is motivated by the presence of skewed distributions and\n",
    "    previously identified outliers, for which median imputation provides\n",
    "    a robust and distribution-preserving solution.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    numeric_cols = df_imputed.select_dtypes(include=\"number\").columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "start_time_mv = now()\n",
    "imputed_data = handle_missing_values(cleaned_data)\n",
    "end_time_mv = now()\n",
    "\n",
    "imputed_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65f980ab-13d7-40c8-a86a-f41d0d6d5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value Provenance\n",
    "\n",
    "mv_ass_uuid_executor = \"2e07bce7-fdb4-4ffe-9992-885d8ac341d8\"\n",
    "\n",
    "handle_missing_executor = [\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_executor} .',\n",
    "    f':{mv_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mv_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(handle_missing_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "mv_ass_uuid_writer = \"7bb13232-573b-4256-96f9-5c741205466f\"\n",
    "\n",
    "mv_comment = \"\"\"\n",
    "Missing values were handled by imputing numeric attributes with their\n",
    "median values. Median imputation was chosen to reduce sensitivity to\n",
    "skewed distributions and outliers commonly observed in educational\n",
    "and cost-related variables. No records were removed at this stage.\n",
    "\"\"\"\n",
    "\n",
    "handle_missing_activity = [\n",
    "    ':handle_missing_values rdf:type prov:Activity .',\n",
    "    ':handle_missing_values sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_missing_values rdfs:comment \"Data Preparation\" .',\n",
    "    f':handle_missing_values rdfs:comment \"\"\"{mv_comment}\"\"\" .',\n",
    "    f':handle_missing_values prov:startedAtTime \"{start_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:endedAtTime \"{end_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_writer} .',\n",
    "    f':{mv_ass_uuid_writer} prov:agent :{handle_missing_code_writer} .',\n",
    "    f':{mv_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_missing_values prov:used :cleaned_data .',\n",
    "    ':imputed_data rdf:type prov:Entity .',\n",
    "    ':imputed_data prov:wasGeneratedBy :handle_missing_values .',\n",
    "    ':imputed_data prov:wasDerivedFrom :cleaned_data .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(handle_missing_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32b97f41-d627-493c-a706-23f7e727cbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Combined_SAT_score</th>\n",
       "      <th>Average_ACT_score</th>\n",
       "      <th>In-state_tuition</th>\n",
       "      <th>Out-of-state_tuition</th>\n",
       "      <th>Room_and_board_costs</th>\n",
       "      <th>Graduation_rate</th>\n",
       "      <th>Number_of_fulltime_undergraduates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>972.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>4120.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>961.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>957.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>881.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>957.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>2550.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3958.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Average_Combined_SAT_score  Average_ACT_score  In-state_tuition  \\\n",
       "0                       972.0               20.0            7560.0   \n",
       "1                       961.0               22.0            1742.0   \n",
       "2                       957.0               22.0            1742.0   \n",
       "3                       881.0               20.0            1742.0   \n",
       "4                       957.0               17.0            1700.0   \n",
       "\n",
       "   Out-of-state_tuition  Room_and_board_costs  Graduation_rate  \\\n",
       "0                7560.0                4120.0             15.0   \n",
       "1                5226.0                3590.0             60.0   \n",
       "2                5226.0                4764.0             39.0   \n",
       "3                5226.0                5120.0             60.0   \n",
       "4                3400.0                2550.0             40.0   \n",
       "\n",
       "   Number_of_fulltime_undergraduates  \n",
       "0                              249.0  \n",
       "1                             3885.0  \n",
       "2                              492.0  \n",
       "3                             6209.0  \n",
       "4                             3958.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Selection\n",
    "\n",
    "selected_columns = [\n",
    "    \"Average_Combined_SAT_score\",\n",
    "    \"Average_ACT_score\",\n",
    "    \"In-state_tuition\",\n",
    "    \"Out-of-state_tuition\",\n",
    "    \"Room_and_board_costs\",\n",
    "    \"Graduation_rate\",\n",
    "    \"Number_of_fulltime_undergraduates\",\n",
    "]\n",
    "\n",
    "prepared_data = imputed_data[selected_columns].copy()\n",
    "prepared_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04e0c6a2-0424-4d8e-a010-e625d41b9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b) Preprocessing steps considered but NOT applied\n",
    "\n",
    "pp_not_applied_comment = \"\"\"\n",
    "Several preprocessing steps were considered but deliberately not applied.\n",
    "Outlier removal was avoided to prevent the exclusion of valid but extreme\n",
    "institutions. Feature scaling and normalization were deferred to the\n",
    "modeling phase, as their necessity depends on algorithm choice. Binning\n",
    "of continuous variables was considered but not applied to preserve numeric\n",
    "precision. Categorical encoding was not required at this stage.\n",
    "\"\"\"\n",
    "\n",
    "pp_not_applied_activity = [\n",
    "    ':preprocessing_not_applied rdf:type prov:Activity .',\n",
    "    ':preprocessing_not_applied sc:isPartOf :data_preparation_phase .',\n",
    "    f':preprocessing_not_applied rdfs:comment \"\"\"{pp_not_applied_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(pp_not_applied_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f6b6500-c395-4c9e-a26b-001d6f0dd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c) Derived attribute analysis\n",
    "\n",
    "derived_attr_comment = \"\"\"\n",
    "Potential derived attributes were analyzed, including cost-per-graduate\n",
    "metrics combining tuition and graduation rate, adjusted student-to-faculty\n",
    "ratios, and composite affordability indicators. While such attributes may\n",
    "provide analytical value, they were not derived at this stage to avoid\n",
    "introducing assumptions without further validation.\n",
    "\"\"\"\n",
    "\n",
    "derived_attr_activity = [\n",
    "    ':derived_attribute_analysis rdf:type prov:Activity .',\n",
    "    ':derived_attribute_analysis sc:isPartOf :data_preparation_phase .',\n",
    "    f':derived_attribute_analysis rdfs:comment \"\"\"{derived_attr_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(derived_attr_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c4cd707-31b8-4193-bc2c-cb7644be6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) External data sources analysis\n",
    "\n",
    "external_data_comment = \"\"\"\n",
    "Additional external data sources could enhance the analysis, such as\n",
    "regional cost-of-living indices, graduate labor market outcomes, or\n",
    "public funding levels by state. These data could improve interpretation\n",
    "of tuition and graduation outcomes and better support the business\n",
    "objectives. Integration was not performed due to scope limitations.\n",
    "\"\"\"\n",
    "\n",
    "external_data_activity = [\n",
    "    ':external_data_analysis rdf:type prov:Activity .',\n",
    "    ':external_data_analysis sc:isPartOf :data_preparation_phase .',\n",
    "    f':external_data_analysis rdfs:comment \"\"\"{external_data_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(external_data_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data prov:wasDerivedFrom :imputed_data .',\n",
    "]\n",
    "\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e22119-a6dd-4b31-84d0-87197ad36b22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"5781efff-7d2a-462b-b855-d52d30ad2a23\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "if PROVENANCE_ENABLED:\n",
    "    engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"6585751d-bec3-40d6-9e9b-f6d4a8431e8c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"37a368b6-f7cc-4184-8417-5938b5ff4a75\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"3d80af2d-0d87-4d93-9cfd-2bbb712714ba\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"5c23b907-faa3-4fb9-93fc-1c9feb2849ad\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"e6484ef9-281e-4d77-9c74-4f9efbc352e9\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"9d08e053-21ce-4c21-8859-a3710db22ea2\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "#if PROVENANCE_ENABLED:\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction done.\n"
     ]
    }
   ],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e07fcd87-655b-413a-92c1-fdf923157a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Data Preparation text extracted.\n"
     ]
    }
   ],
   "source": [
    "dp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT DISTINCT ?comment WHERE {{\n",
    "  ?e sc:isPartOf :data_preparation_phase .\n",
    "  ?e rdfs:comment ?comment .\n",
    "  FILTER (\n",
    "    STRLEN(STR(?comment)) > 50 &&\n",
    "    !CONTAINS(LCASE(STR(?comment)), \"data preparation\")\n",
    "  )\n",
    "}}\n",
    "ORDER BY ?comment\n",
    "\"\"\"\n",
    "\n",
    "res_dp = engine.query(dp_query)\n",
    "\n",
    "if res_dp.empty:\n",
    "    dp_text = \"No data preparation documentation available.\"\n",
    "else:\n",
    "    dp_text = \"\\n\\n\".join(\n",
    "        latex_escape(clean_rdf(c))\n",
    "        for c in res_dp[\"comment\"]\n",
    "    )\n",
    "\n",
    "print(\"Clean Data Preparation text extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "{dp_text}\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to: data\\report\\experiment_report.tex\n"
     ]
    }
   ],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d68f3c-f257-4b0d-bb8f-0de3b34b7262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BI2025)",
   "language": "python",
   "name": "bi2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
